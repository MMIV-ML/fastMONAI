{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'fastMONAI: a low-code deep learning library for medical image analysis'\n",
    "tags:\n",
    "  - deep learning\n",
    "  - medical imaging\n",
    "  - PyTorch\n",
    "  - fastai\n",
    "  - MONAI\n",
    "  - torchIO\n",
    "authors:\n",
    "  - name: Satheshkumar Kaliyugarasan\n",
    "    orcid: 0000-0002-0038-5540\n",
    "    affiliation: \"1,2\" \n",
    "    \n",
    "  - name: Alexander Selvikvåg Lundervold\n",
    "    orcid: 0000-0001-8663-4247\n",
    "    affiliation: \"1,2\" \n",
    "affiliations:\n",
    " - name: Department of Computer Science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, Bergen, Norway\n",
    "   index: 1\n",
    " - name: Mohn Medical Imaging and Visualization Centre, Department of Radiology, Haukeland University Hospital, Bergen, Norway\n",
    "   index: 2\n",
    "date: January 2023\n",
    "bibliography: paper.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/fastMONAI/blob/master/paper/paper.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> _\"Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\"_[@donaldknuth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we present <b>fastMONAI</b>, a low-code Python-based open source deep learning library built on top of fastai [@howard2020fastai; @howard2020deep], MONAI [@monai], and TorchIO [@perez2021torchio]. We created the library to simplify the use of state-of-the-art deep learning techniques in 3D medical image analysis for solving classification, regression, and segmentation tasks. fastMONAI provides users with functionalities to step through data loading, preprocessing, training, and result interpretations.\n",
    "\n",
    "The paper is structured in the following way: it first states the need for the research, then showcases various applications and the library's user-friendliness, followed by a discussion about documentation, usability, and maintainability.\n",
    "\n",
    "Note that this paper is automatically generated from a Jupyter Notebook available in the fastMONAI GitHub repo: [https://github.com/MMIV-ML/fastMONAI](https://github.com/MMIV-ML/fastMONAI). Using the notebook makes it possible to step through the paper's content and reproduce all the computations and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statement of need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning develops at breakneck speed, with new models, techniques, and tricks constantly appearing. As a result, it is easy to get stuck on something less-than-optimal when using deep learning to solve a particular set of problems while also being in danger of getting lost in minor technical details when constructing models for concrete tasks. The fastai deep learning library [@howard2020fastai; @howard2020deep] provides both a high-level API that automatically incorporates many established best practices and a low-level API in which one can modify details related to model architectures, training strategies, data augmentation, and more. \n",
    "\n",
    "fastai is a general deep learning library built on top of PyTorch. Healthcare imaging has a variety of domain-specific demands, including medical imaging formats, data storage and transfer, data labeling procedures, domain-specific data augmentation, and evaluation methods. MONAI Core [@monai] and TorchIO [@perez2021torchio] target deep learning in healthcare imaging, incorporating multiple best practices. MONAI Core, the primary library of Project MONAI, is built on top of PyTorch and provides domain-specific functionalities for medical imaging, including network architectures, metrics, and loss functions. \n",
    "TorchIO is a Python-based open-source library for efficiently loading, preprocessing, and augmenting 3D medical images. \n",
    "\n",
    "A visual representation learning system is determined by three key factors: network architecture chosen, training methods, and data [@woo2023convnext]. Our combination of fastai, MONAI Core, and TorchIO into fastMONAI with custom modules like MedDataset makes it possible to easily construct, use and train powerful models for a range of medical imaging tasks, using all the best practices and domain-specific features incorporated into these three libraries. The library is developed at The Mohn Medical Imaging and Visualization Centre (MMIV), which is part of the Department of Radiology at Haukeland University Hospital. One of the center's key objectives is to develop new quantitative methods for high-field MRI, CT, and hybrid PET/CT/MR in preclinical and clinical settings, aiming to improve decision-making and patient care. fastMONAI supports such efforts by easing the entry for new practitioners into medical AI and making it possible to quickly construct good baseline models while still being flexible enough to enable further optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fastMONAI\n",
    "\n",
    "In this section, we will explore how to use our library. In fastMONAI's online documentation [https://fastmonai.no](https://fastmonai.no), multiple tutorials cover classification, regression, and segmentation tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the library, the first step is to import the necessary functions and classes. For example, the following line imports all of the functions and classes from the fastMONAI library: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "from fastMONAI.vision_all import *\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the use of fastMONAI, we download the MedMNIST lung nodule (REF) data with corresponding labels, indicating whether the nodules are benign (b) or malignant (m):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "df, _ = download_NoduleMNIST3D(max_workers=8)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the processed DataFrame is formatted:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "print(df.head(1).to_markdown())\n",
    "```\n",
    "==================================================\n",
    "\n",
    "|                                           img_path | labels | is_val |\n",
    "|---------------------------------------------------:|-------:|-------:|\n",
    "| ../data/NoduleMNIST3D/train_images/0_nodule.nii.gz |      b |  False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fastMONAI, various data augmentation techniques are available for training vision models, and they can also optionally be applied during inference.\n",
    "Data augmentation is a vital regularization technique in training vision models, which aims to expand the diversity of a given dataset by performing random, realistic transformations such\n",
    "as rotation, zoom, and others). The following code cell specifies a list of transformations to be applied to the items in the training set. The complete list of available transformations in the library can be found at [https://fastmonai.no/vision_augment](https://fastmonai.no/vision_augment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "item_tfms=[PadOrCrop(size=28), RandomAffine(degrees=35, isotropic=True), \n",
    "           ZNormalization()]\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the data into a model, we must create a `DataLoaders` object for our dataset. There are several ways to get the data in `DataLoaders`. \n",
    "In the following line, we call the ` ImageDataLoaders.from_df` factory method, which is the most basic way of building a `DataLoaders`. \n",
    "\n",
    "Here, we pass the processed DataFrame, define the columns for the images `fn_col` and the labels `label_col`, some transforms `item_tfms`, voxel spacing `resample`, and the batch size `bs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "dls = MedImageDataLoaders.from_df(df, fn_col='img_path', \n",
    "                                  label_col='labels', \n",
    "                                  item_tfms=item_tfms, \n",
    "                                  resample=1,\n",
    "                                  bs=64)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at a batch of images in the training set using `show_batch` :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "dls.show_batch(max_n=2, anatomical_plane=2)\n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_22_0.png){ width=30% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Class imbalance_ is a common challenge in medical datasets, and it is something we're facing in our example data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b    986\n",
      "m    337\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to deal with class imbalance. A straightforward technique is to use balancing weights in the model's loss function, i.e., penalizing misclassifications for instances belonging to the minority class more heavily than those of the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6709, 1.9627])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_train = df.loc[~df.is_val].labels\n",
    "weights = torch.Tensor(compute_class_weight(class_weight='balanced', \n",
    "                                            classes=np.unique(y_train),\n",
    "                                            y=y_train.values.reshape(-1)))\n",
    "\n",
    "print(weights)\n",
    "\n",
    "loss_func = CrossEntropyLossFlat(weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to construct a deep learning classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a 3D deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import a classification network from MONAI and configure it based on our task, including defining the input image size, the number of classes to predict, channels, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "from monai.networks.nets import Classifier\n",
    "\n",
    "model = Classifier(in_shape=[1, 28, 28, 28], classes=2, \n",
    "                   channels=(8, 16, 32),strides=(2, 2, 2, 2))\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a `Learner`, which is a fastai object that combines the data and our defined model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "learn = Learner(dls, model,loss_func=loss_func, metrics=accuracy)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "learn.fit_one_cycle(4) \n",
    "```\n",
    "==================================================\n",
    "\n",
    "| epoch | train_loss | valid_loss | accuracy |  time |\n",
    "|------:|-----------:|-----------:|---------:|------:|\n",
    "|     0 |   0.583369 |   0.475026 | 0.821970 | 00:03 |\n",
    "|     1 |   0.523478 |   0.467598 | 0.750000 | 00:02 |\n",
    "|     2 |   0.470980 |   0.448630 | 0.818182 | 00:02 |\n",
    "|     3 |   0.428916 |   0.444447 | 0.829545 | 00:02 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Small random variations are involved in training CNN models. Hence, when running the notebook, you may see different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained, let's look at some predictions on the validation data. The `show_results` method plots instances, their target values, and their corresponding predictions from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "learn.show_results(max_n=2, anatomical_plane=2) \n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_38_2.png){ width=30% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how often and for what instances our trained model becomes confused while making predictions on the validation data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "interp.plot_confusion_matrix()\n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_44_2.png){ width=30% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two instances our model was most confused about (in other words, most confident but wrong):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "interp.plot_top_losses(k=2, anatomical_plane=2)\n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_43_2.png){ width=30% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving results using test-time augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-time augmentation (TTA) is a technique where you apply data augmentation transforms when making predictions to produce average output. In addition to often yielding better performance, the variation in the output of the TTA runs can provide some measure of its robustness and sensitivity to augmentations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBase(0.8371)\n"
     ]
    }
   ],
   "source": [
    "preds, targs = learn.tta(n=4); \n",
    "print(accuracy(preds, targs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we look at another computer vision task while also taking a closer look at the fastMONAI library. Our task will be _semantic segmentation_, and we'll use the IXI Tiny dataset (a small version of the IXI dataset). In semantic segmentation, a class label is assigned to each pixel or voxel in an image, in this case, distinguishing brain tissue from non-brain tissue, i.e., skull-stripping or brain extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "path = Path('../data')\n",
    "STUDY_DIR = download_ixi_tiny(path=path)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "df = pd.read_csv(STUDY_DIR/'dataset.csv')\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastMONAI class `MedDataset` can automatically extract and present valuable information about your dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "med_dataset = MedDataset(path=STUDY_DIR/'image', reorder=True, max_workers=6)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "data_info_df = med_dataset.summary()\n",
    "print(data_info_df.head().to_markdown())\n",
    "\n",
    "```\n",
    "==================================================\n",
    "\n",
    "| dim_0 | dim_1 | dim_2 | voxel_0 | voxel_1 | voxel_2 | orientation | example_path |                                               total |     |\n",
    "|------:|------:|------:|--------:|--------:|--------:|------------:|-------------:|----------------------------------------------------:|-----|\n",
    "|    44 |    55 |    83 |    4.13 |    3.95 |    2.18 |        RAS+ |         RAS+ | ../data/IXITiny/image/IXI002-Guys-0828_image.nii.gz | 566 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.13, 3.95, 2.18] True\n"
     ]
    }
   ],
   "source": [
    "resample, reorder = med_dataset.suggestion()\n",
    "print(resample, reorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the largest image size in the dataset with the recommended resampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.0, 55.0, 83.0]\n"
     ]
    }
   ],
   "source": [
    "img_size = med_dataset.get_largest_img_size(resample=resample)\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we choose the following size as some network architectues requires the tensor to be divisible by 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "size = [48, 48, 96]\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "item_tfms = [PadOrCrop(size), \n",
    "             RandomAffine(scales=0.1, degrees=5, p=0.5), RandomFlip(p=0.5), \n",
    "             ZNormalization()] \n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, there are several ways to get the data in `DataLoaders`. In this section, let's build the data loaders using `DataBlock`. \n",
    "Here we need to define what our input and target should be (`MedImage` and `MedMaskBlock` for segmentation), how to get the images and the labels, how to split the data, item transforms that should be applied during training, reorder voxel orientations, and voxel spacing. Take a look at fastai's documentation for DataBlock for further information: [https://docs.fast.ai/data.block.html#DataBlock](https://docs.fast.ai/data.block.html#DataBlock).\n",
    "\n",
    "**NB:** It is crucial to select an appropriate splitting strategy. For example, one should, in general, avoid having data from the same patient in both the training and the validation or test set. However, in the IXI data set this is not an issue, as there is only one image per patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "dblock = MedDataBlock(blocks=(ImageBlock(cls=MedImage), MedMaskBlock), \n",
    "                      splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "                      get_x=ColReader('t1_path'),\n",
    "                      get_y=ColReader('labels'),\n",
    "                      item_tfms=item_tfms,\n",
    "                      reorder=reorder,\n",
    "                      resample=resample) \n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass our processed DataFrame and the batch size (bs) to create a `DataLoaders` object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "dls = dblock.dataloaders(df, bs=8)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "dls.show_batch(max_n=2, anatomical_plane=2)\n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_70_0.png){ width=30% }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 109)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dls.train_ds.items), len(dls.valid_ds.items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architectures and loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import various models and loss functions directly from MONAI Core, as shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "from monai.networks.nets import UNet, AttentionUnet\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "\n",
    "loss_func = CustomLoss(loss_func=DiceFocalLoss(sigmoid=True))\n",
    "\n",
    "model = AttentionUnet(spatial_dims=3, in_channels=1, out_channels=1, \n",
    "                channels=(16, 32, 64, 128), strides=(2, 2, 2))\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```pyhton\n",
    "learn = Learner(dls, model, loss_func=loss_func, opt_func=ranger, \n",
    "                metrics=[binary_dice_score, binary_hausdorff_distance])\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a good learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the default learning rate before, but we might want to find a better value. For this, we can use the learning rate finder of fastai:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```pyhton\n",
    "lr = learn.lr_find()\n",
    "```\n",
    "==================================================\n",
    "\n",
    "![](paper_files/output_79_2.png){ width=40% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model. We again use the one-cycle learning rate policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "learn.fit_one_cycle(2, lr.valley)\n",
    "learn.save('model-1')\n",
    "```\n",
    "==================================================\n",
    "\n",
    "| epoch | train_loss | valid_loss | binary_dice_score | binary_hausdorff_distance |  time |\n",
    "|------:|-----------:|-----------:|------------------:|--------------------------:|------:|\n",
    "|     0 |   0.522038 |   0.403086 |          0.928434 |                  6.923530 | 00:15 |\n",
    "|     1 |   0.419204 |   0.376280 |          0.952868 |                  4.977974 | 00:15 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting and sharing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export the model and share both the trained weights and the learner on [HuggingFace](https://huggingface.co/docs/hub/repositories-getting-started) and use tagging for marked version release. Version control for shared models is essential for tracking changes and being able to roll back to previous versions if there are any issues with the latest model in production.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "```python\n",
    "learn.export('models/export.pkl')\n",
    "store_variables(pkl_fn='models/vars.pkl', size=size, \n",
    "                reorder=reorder, resample=resample)\n",
    "```\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation, usability, and maintainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written the entire fastMONAI library using nbdev, a tool for exploratory programming that allows you to write, test, and document a Python library in Jupyter Notebooks. fastMONAI contains several practical tools to ensure the software's user-friendliness. \n",
    "\n",
    "fastMONAI comes with a documentation page [https://fastmonai.no](https://fastmonai.no) and step-by-step tutorials on how to use the software for various medical imaging tasks (e.g., classification, regression, and segmentation). Tests are written directly in notebooks, and continuous integration with GitHub Actions runs the tests on each push, making software development easier with multiple collaborators. \n",
    "\n",
    "To ease further extensions of our library through contributions, we have added a short guide on how to contribute to the project. As mentioned, this paper is written as a notebook and automatically converted to a markdown file. The latest version is always available on GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research projects using fastMONAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastMONAI library has been used for various medical imaging tasks, including predicting brain age using T1-weighted scans in [@kaliyugarasan2020brain], skull-stripping in [@kaliyugarasan20202d], pulmonary nodule classification from CT images in [@kaliyugarasan2021pulmonary], and tumor segmentation in cervical cancer from multi-parametric pelvic MRI in [@hodneland2022fully]. Recently, it was also used for vertebra segmentation in a multi-center study [@kaliyugarasan2023spine]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trond Mohn Research Foundation supported our work through the project _“Computational medical imaging and machine learning - methods, infrastructure and applications”_ at the Mohn Medical Imaging and Visualization Centre, grant number BFS2018TMT07, and a grant from the Western Norway Regional Health Authority (Helse Vest RHF), project F-12532."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastmonai",
   "language": "python",
   "name": "fastmonai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
