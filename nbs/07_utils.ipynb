{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339eff2-b4a5-46c8-9622-f5d123f5338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac941446-cf7e-4f5c-ace0-a36fb078022f",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nimport pickle\nimport torch\nfrom pathlib import Path\nimport mlflow\nimport mlflow.pytorch\nimport os\nimport tempfile\nimport json\nfrom fastai.callback.core import Callback\nfrom fastcore.foundation import L\nfrom typing import Any"
  },
  {
   "cell_type": "markdown",
   "id": "60ea67a2-281a-4c96-9869-6e1a1050f44b",
   "metadata": {},
   "source": [
    "# Utils\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f954a64-c9df-4ee8-b70a-fce7f857eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def store_variables(pkl_fn: str | Path, size: list, reorder: bool, resample: int | list):\n",
    "    \"\"\"Save variable values in a pickle file.\"\"\"\n",
    "    \n",
    "    var_vals = [size, reorder, resample]\n",
    "    \n",
    "    with open(pkl_fn, 'wb') as f:\n",
    "        pickle.dump(var_vals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db5512-171c-4dfd-a26e-561b773a6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_variables(pkl_fn: (str, Path)):\n",
    "    \"\"\"Loads stored variable values from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        pkl_fn: File path of the pickle file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        The deserialized value of the pickled data.\n",
    "    \"\"\"\n",
    "    with open(pkl_fn, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1513a-25d6-497a-bec3-6adc008452d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_colab_gpu_info(): \n",
    "    \"\"\"Check if we have a GPU attached to the runtime.\"\"\"\n",
    "    \n",
    "    colab_gpu_msg =(f\"{'#'*80}\\n\"\n",
    "                    \"Remember to attach a GPU to your Colab Runtime:\"\n",
    "                    \"\\n1. From the **Runtime** menu select **Change Runtime Type**\"\n",
    "                    \"\\n2. Choose **GPU** from the drop-down menu\"\n",
    "                    \"\\n3. Click **'SAVE'**\\n\"\n",
    "                    f\"{'#'*80}\")\n",
    "    \n",
    "    if torch.cuda.is_available(): print('GPU attached.')\n",
    "    else: print(colab_gpu_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030876d2",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nclass ModelTrackingCallback(Callback):\n    \"\"\"\n    A FastAI callback for comprehensive MLflow experiment tracking.\n    \n    This callback automatically logs hyperparameters, metrics, model artifacts,\n    and configuration to MLflow during training.\n    \"\"\"\n    \n    def __init__(\n        self, \n        model_name: str, \n        loss_function: str, \n        item_tfms: list[Any],\n        size: list[int], \n        resample: list[float], \n        reorder: bool\n    ):\n        \"\"\"\n        Initialize the MLflow tracking callback.\n        \n        Args:\n            model_name: Name of the model architecture for registration\n            loss_function: Name of the loss function being used\n            size: Model input dimensions\n            resample: Resampling dimensions\n            reorder: Whether reordering augmentation is applied\n        \"\"\"\n        self.model_name = model_name\n        self.loss_function = loss_function\n        self.item_tfms = item_tfms\n        self.size = size\n        self.resample = resample\n        self.reorder = reorder\n        \n        self.config = self._build_config()\n        \n    def extract_all_params(self, tfm):\n        \"\"\"\n        Extract all parameters from a transform object for detailed logging.\n        \n        Args:\n            tfm: Transform object to extract parameters from\n            \n        Returns:\n            dict: Dictionary with 'name' and 'params' keys containing transform details\n        \"\"\"\n        class_name = tfm.__class__.__name__\n        params = {}\n        \n        for key, value in tfm.__dict__.items():\n            if not key.startswith('_') and key != '__signature__':\n                if hasattr(value, '__dict__') and hasattr(value, 'target_shape'):\n                    params['target_shape'] = value.target_shape\n                elif hasattr(value, '__dict__') and not key.startswith('_'):\n                    nested_params = {k: v for k, v in value.__dict__.items() \n                                   if not k.startswith('_') and isinstance(v, (int, float, str, bool, tuple, list))}\n                    params.update(nested_params)\n                elif isinstance(value, (int, float, str, bool, tuple, list)):\n                    params[key] = value\n        \n        return {\n            'name': class_name,\n            'params': params\n        }\n        \n    def _build_config(self) -> dict[str, Any]:\n        \"\"\"Build configuration dictionary from initialization parameters.\"\"\"\n        # Extract detailed transform information\n        transform_details = [self.extract_all_params(tfm) for tfm in self.item_tfms]\n        \n        return {\n            \"model_name\": self.model_name,\n            \"loss_function\": self.loss_function,\n            \"transform_details\": transform_details,\n            \"size\": self.size,\n            \"resample\": self.resample,\n            \"reorder\": self.reorder,\n        }\n    \n    def _extract_training_params(self) -> dict[str, Any]:\n        \"\"\"Extract training hyperparameters from the learner.\"\"\"\n        params = {}\n        \n        params[\"epochs\"] = self.learn.n_epoch\n        params[\"learning_rate\"] = float(self.learn.lr)\n        params[\"optimizer\"] = self.learn.opt_func.__name__\n        params[\"batch_size\"] = self.learn.dls.bs\n        \n        params[\"loss_function\"] = self.config[\"loss_function\"]\n        params[\"size\"] = self.config[\"size\"]\n        params[\"resample\"] = self.config[\"resample\"]\n        params[\"reorder\"] = self.config[\"reorder\"]\n        \n        params[\"transformations\"] = json.dumps(\n            self.config[\"transform_details\"], \n            indent=2, \n            separators=(',', ': ')\n        )\n        \n        return params\n    \n    def _extract_epoch_metrics(self) -> dict[str, float]:\n        \"\"\"Extract metrics from the current epoch.\"\"\"\n        recorder = self.learn.recorder\n        \n        # Get custom metric names and values (skip 'epoch' and 'time')\n        metric_names = recorder.metric_names[2:]\n        raw_metric_values = recorder.log[2:]\n        \n        metrics = {}\n        \n        # Process each metric, handling both scalars and tensors\n        for name, val in zip(metric_names, raw_metric_values):\n            if val is None:\n                continue  # Skip None values during inference\n            if isinstance(val, torch.Tensor):\n                if val.numel() == 1:\n                    # Single value tensor (like binary dice score)\n                    metrics[name] = float(val)\n                else:\n                    # Multi-element tensor (like multiclass dice scores)\n                    val_list = val.tolist() if hasattr(val, 'tolist') else list(val)\n                    # Log individual class scores\n                    for i, class_score in enumerate(val_list):\n                        metrics[f\"{name}_class_{i+1}\"] = float(class_score)\n                    # Log mean across classes\n                    metrics[f\"{name}_mean\"] = float(torch.mean(val))\n            else:\n                metrics[name] = float(val)\n        \n        # Handle loss values\n        if len(recorder.log) >= 2:\n            if recorder.log[1] is not None:\n                metrics['train_loss'] = float(recorder.log[1])\n            if len(recorder.log) >= 3 and recorder.log[2] is not None:\n                metrics['valid_loss'] = float(recorder.log[2])\n                \n        return metrics\n    \n    def _save_model_artifacts(self, temp_dir: Path) -> None:\n        \"\"\"Save model weights, learner, and configuration as artifacts.\"\"\"\n        weights_path = temp_dir / \"weights\"\n        self.learn.save(str(weights_path))\n        \n        weights_file = f\"{weights_path}.pth\"\n        if os.path.exists(weights_file):\n            mlflow.log_artifact(weights_file, \"model\")\n        \n        # Remove MLflow callbacks before exporting learner for inference\n        # This prevents the callback from being triggered during inference\n        original_cbs = self.learn.cbs.copy()  # Save original callbacks\n        \n        # Remove ModelTrackingCallback instances from learner using proper collection type\n        filtered_cbs = L([cb for cb in self.learn.cbs if not isinstance(cb, ModelTrackingCallback)])\n        self.learn.cbs = filtered_cbs\n        \n        # Export clean learner without MLflow callbacks\n        learner_path = temp_dir / \"learner.pkl\"\n        self.learn.export(str(learner_path))\n        mlflow.log_artifact(str(learner_path), \"model\")\n        \n        # Restore original callbacks for current session\n        self.learn.cbs = original_cbs\n        \n        config_path = temp_dir / \"inference_settings.pkl\"\n        store_variables(config_path, self.size, self.reorder, self.resample)\n        mlflow.log_artifact(str(config_path), \"config\")\n    \n    def _register_pytorch_model(self) -> None:\n        \"\"\"Register the PyTorch model with MLflow.\"\"\"\n        mlflow.pytorch.log_model(\n            pytorch_model=self.learn.model,\n            registered_model_name=self.model_name\n        )\n    \n    def before_fit(self) -> None:\n        \"\"\"Log hyperparameters before training starts.\"\"\"\n        params = self._extract_training_params()\n        mlflow.log_params(params)\n    \n    def after_epoch(self) -> None:\n        \"\"\"Log metrics after each epoch.\"\"\"\n        metrics = self._extract_epoch_metrics()\n        if metrics:\n            mlflow.log_metrics(metrics, step=self.learn.epoch)\n    \n    def after_fit(self) -> None:\n        \"\"\"Log model artifacts after training completion.\"\"\"\n        print(\"\\nTraining finished. Logging model artifacts to MLflow...\")\n        \n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n            \n            self._save_model_artifacts(temp_path)\n            \n            self._register_pytorch_model()\n            \n        print(f\"MLflow run completed. Run ID: {mlflow.active_run().info.run_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import os\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from IPython.core.magic import register_line_magic\n",
    "from IPython import get_ipython\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "class MLflowUIManager:\n",
    "    def __init__(self):\n",
    "        self.process = None\n",
    "        self.thread = None\n",
    "        self.port = 5001\n",
    "        self.host = '0.0.0.0'\n",
    "        self.backend_store_uri = './mlruns'\n",
    "        \n",
    "    def is_port_available(self, port):\n",
    "        \"\"\"Check if a port is available.\"\"\"\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('localhost', port))\n",
    "                return True\n",
    "            except OSError:\n",
    "                return False\n",
    "    \n",
    "    def is_mlflow_running(self):\n",
    "        \"\"\"Check if MLflow UI is actually responding.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f'http://localhost:{self.port}', timeout=2)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def find_available_port(self, start_port=5001):\n",
    "        \"\"\"Find an available port starting from start_port.\"\"\"\n",
    "        for port in range(start_port, start_port + 10):\n",
    "            if self.is_port_available(port):\n",
    "                return port\n",
    "        return None\n",
    "    \n",
    "    def check_mlflow_installed(self):\n",
    "        \"\"\"Check if MLflow is installed.\"\"\"\n",
    "        return shutil.which('mlflow') is not None\n",
    "    \n",
    "    def start_ui(self, auto_open=True, quiet=False):\n",
    "        \"\"\"Start MLflow UI with better error handling and user feedback.\"\"\"\n",
    "        \n",
    "        # Check if MLflow is installed\n",
    "        if not self.check_mlflow_installed():\n",
    "            if not quiet:\n",
    "                display(HTML('<div style=\"color: #d32f2f; font-weight: bold; font-size: 14px;\">❌ MLflow not installed. Run: pip install mlflow</div>'))\n",
    "            return False\n",
    "        \n",
    "        # Find available port\n",
    "        available_port = self.find_available_port(self.port)\n",
    "        if available_port is None:\n",
    "            if not quiet:\n",
    "                display(HTML('<div style=\"color: #d32f2f; font-weight: bold; font-size: 14px;\">❌ No available ports found (5001-5010)</div>'))\n",
    "            return False\n",
    "        \n",
    "        self.port = available_port\n",
    "        \n",
    "        # Start MLflow UI in a separate thread\n",
    "        def run_mlflow():\n",
    "            try:\n",
    "                self.process = subprocess.Popen([\n",
    "                    'mlflow', 'ui', \n",
    "                    '--host', self.host,\n",
    "                    '--port', str(self.port),\n",
    "                    '--backend-store-uri', self.backend_store_uri\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                self.process.wait()\n",
    "            except Exception as e:\n",
    "                if not quiet:\n",
    "                    display(HTML(f'<div style=\"color: #d32f2f; font-weight: bold; font-size: 14px;\">❌ Error: {str(e)}</div>'))\n",
    "        \n",
    "        self.thread = threading.Thread(target=run_mlflow, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "        # Wait and check if server started successfully\n",
    "        max_wait = 10\n",
    "        for i in range(max_wait):\n",
    "            time.sleep(1)\n",
    "            if self.is_mlflow_running():\n",
    "                if quiet:\n",
    "                    # Bright, visible link for quiet mode\n",
    "                    display(HTML(f'''\n",
    "                        <a href=\"http://localhost:{self.port}\" target=\"_blank\" \n",
    "                           style=\"color: #1976d2; font-weight: bold; font-size: 16px; text-decoration: underline;\">\n",
    "                           🔗 MLflow UI (Port {self.port})\n",
    "                        </a>\n",
    "                    '''))\n",
    "                else:\n",
    "                    # Success message with high contrast colors\n",
    "                    display(HTML(f'''\n",
    "                        <div style=\"background-color: #c8e6c9; border: 2px solid #388e3c; padding: 15px; border-radius: 8px; margin: 10px 0;\">\n",
    "                            <div style=\"color: #1b5e20; font-weight: bold; font-size: 16px; margin-bottom: 10px;\">\n",
    "                                ✅ MLflow UI is running successfully!\n",
    "                            </div>\n",
    "                            <a href=\"http://localhost:{self.port}\" target=\"_blank\" \n",
    "                               style=\"background-color: #1976d2; color: white; padding: 12px 24px; text-decoration: none; border-radius: 6px; font-weight: bold; font-size: 14px; display: inline-block; margin: 5px 0;\">\n",
    "                                🔗 Open MLflow UI\n",
    "                            </a>\n",
    "                            <div style=\"margin-top: 10px;\">\n",
    "                                <div style=\"color: #424242; font-size: 13px;\">URL: http://localhost:{self.port}</div>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                    '''))\n",
    "                return True\n",
    "        \n",
    "        # If we get here, server didn't start properly\n",
    "        if not quiet:\n",
    "            display(HTML('<div style=\"color: #d32f2f; font-weight: bold; font-size: 14px;\">❌ Failed to start MLflow UI</div>'))\n",
    "        return False\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the MLflow UI server.\"\"\"\n",
    "        if self.process:\n",
    "            self.process.terminate()\n",
    "            self.process = None\n",
    "            display(HTML('''\n",
    "                <div style=\"background-color: #ffecb3; border: 2px solid #f57c00; padding: 10px; border-radius: 6px;\">\n",
    "                    <span style=\"color: #e65100; font-weight: bold; font-size: 14px;\">🛑 MLflow UI stopped</span>\n",
    "                </div>\n",
    "            '''))\n",
    "        else:\n",
    "            display(HTML('''\n",
    "                <div style=\"background-color: #f0f0f0; border: 2px solid #757575; padding: 10px; border-radius: 6px;\">\n",
    "                    <span style=\"color: #424242; font-weight: bold; font-size: 14px;\">ℹ️ MLflow UI is not currently running</span>\n",
    "                </div>\n",
    "            '''))\n",
    "    \n",
    "    def status(self):\n",
    "        \"\"\"Check MLflow UI status.\"\"\"\n",
    "        if self.is_mlflow_running():\n",
    "            display(HTML(f'''\n",
    "                <div style=\"background-color: #c8e6c9; border: 2px solid #388e3c; padding: 10px; border-radius: 6px;\">\n",
    "                    <div style=\"color: #1b5e20; font-weight: bold; font-size: 14px;\">✅ MLflow UI is running</div>\n",
    "                    <a href=\"http://localhost:{self.port}\" target=\"_blank\" \n",
    "                       style=\"color: #1976d2; font-weight: bold; text-decoration: underline;\">\n",
    "                       http://localhost:{self.port}\n",
    "                    </a>\n",
    "                </div>\n",
    "            '''))\n",
    "        else:\n",
    "            display(HTML('''\n",
    "                <div style=\"background-color: #ffcdd2; border: 2px solid #d32f2f; padding: 10px; border-radius: 6px;\">\n",
    "                    <div style=\"color: #b71c1c; font-weight: bold; font-size: 14px;\">❌ MLflow UI is not running</div>\n",
    "                    <div style=\"color: #424242; font-size: 13px; margin-top: 5px;\">\n",
    "                        Run <code style=\"background-color: #f5f5f5; padding: 2px 4px; border-radius: 3px;\">mlflow_ui.start_ui()</code> to start it.\n",
    "                    </div>\n",
    "                </div>\n",
    "            '''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastmonai",
   "language": "python",
   "name": "fastmonai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
