{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339eff2-b4a5-46c8-9622-f5d123f5338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac941446-cf7e-4f5c-ace0-a36fb078022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "from fastai.callback.core import Callback\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea67a2-281a-4c96-9869-6e1a1050f44b",
   "metadata": {},
   "source": [
    "# Utils\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f954a64-c9df-4ee8-b70a-fce7f857eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def store_variables(pkl_fn: str | Path, size: list, reorder: bool, resample: int | list):\n",
    "    \"\"\"Save variable values in a pickle file.\"\"\"\n",
    "    \n",
    "    var_vals = [size, reorder, resample]\n",
    "    \n",
    "    with open(pkl_fn, 'wb') as f:\n",
    "        pickle.dump(var_vals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db5512-171c-4dfd-a26e-561b773a6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_variables(pkl_fn: (str, Path)):\n",
    "    \"\"\"Loads stored variable values from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        pkl_fn: File path of the pickle file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        The deserialized value of the pickled data.\n",
    "    \"\"\"\n",
    "    with open(pkl_fn, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1513a-25d6-497a-bec3-6adc008452d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_colab_gpu_info(): \n",
    "    \"\"\"Check if we have a GPU attached to the runtime.\"\"\"\n",
    "    \n",
    "    colab_gpu_msg =(f\"{'#'*80}\\n\"\n",
    "                    \"Remember to attach a GPU to your Colab Runtime:\"\n",
    "                    \"\\n1. From the **Runtime** menu select **Change Runtime Type**\"\n",
    "                    \"\\n2. Choose **GPU** from the drop-down menu\"\n",
    "                    \"\\n3. Click **'SAVE'**\\n\"\n",
    "                    f\"{'#'*80}\")\n",
    "    \n",
    "    if torch.cuda.is_available(): print('GPU attached.')\n",
    "    else: print(colab_gpu_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030876d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelTrackingCallback(Callback):\n",
    "    \"\"\"\n",
    "    A FastAI callback for comprehensive MLflow experiment tracking.\n",
    "    \n",
    "    This callback automatically logs hyperparameters, metrics, model artifacts,\n",
    "    and configuration to MLflow during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        loss_function: str, \n",
    "        item_tfms: list[Any],\n",
    "        size: list[int], \n",
    "        resample: list[float], \n",
    "        reorder: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MLflow tracking callback.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model architecture for registration\n",
    "            loss_function: Name of the loss function being used\n",
    "            size: Model input dimensions\n",
    "            resample: Resampling dimensions\n",
    "            reorder: Whether reordering augmentation is applied\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.loss_function = loss_function\n",
    "        self.item_tfms = item_tfms\n",
    "        self.size = size\n",
    "        self.resample = resample\n",
    "        self.reorder = reorder\n",
    "        \n",
    "        self.config = self._build_config()\n",
    "        \n",
    "    def extract_all_params(self, tfm):\n",
    "        \"\"\"\n",
    "        Extract all parameters from a transform object for detailed logging.\n",
    "        \n",
    "        Args:\n",
    "            tfm: Transform object to extract parameters from\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with 'name' and 'params' keys containing transform details\n",
    "        \"\"\"\n",
    "        class_name = tfm.__class__.__name__\n",
    "        params = {}\n",
    "        \n",
    "        for key, value in tfm.__dict__.items():\n",
    "            if not key.startswith('_') and key != '__signature__':\n",
    "                if hasattr(value, '__dict__') and hasattr(value, 'target_shape'):\n",
    "                    params['target_shape'] = value.target_shape\n",
    "                elif hasattr(value, '__dict__') and not key.startswith('_'):\n",
    "                    nested_params = {k: v for k, v in value.__dict__.items() \n",
    "                                   if not k.startswith('_') and isinstance(v, (int, float, str, bool, tuple, list))}\n",
    "                    params.update(nested_params)\n",
    "                elif isinstance(value, (int, float, str, bool, tuple, list)):\n",
    "                    params[key] = value\n",
    "        \n",
    "        return {\n",
    "            'name': class_name,\n",
    "            'params': params\n",
    "        }\n",
    "        \n",
    "    def _build_config(self) -> dict[str, Any]:\n",
    "        \"\"\"Build configuration dictionary from initialization parameters.\"\"\"\n",
    "        # Extract detailed transform information\n",
    "        transform_details = [self.extract_all_params(tfm) for tfm in self.item_tfms]\n",
    "        \n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"loss_function\": self.loss_function,\n",
    "            \"transform_details\": transform_details,\n",
    "            \"size\": self.size,\n",
    "            \"resample\": self.resample,\n",
    "            \"reorder\": self.reorder,\n",
    "        }\n",
    "    \n",
    "    def _extract_training_params(self) -> dict[str, Any]:\n",
    "        \"\"\"Extract training hyperparameters from the learner.\"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        params[\"epochs\"] = self.learn.n_epoch\n",
    "        params[\"learning_rate\"] = float(self.learn.lr)\n",
    "        params[\"optimizer\"] = self.learn.opt_func.__name__\n",
    "        params[\"batch_size\"] = self.learn.dls.bs\n",
    "        \n",
    "        params[\"loss_function\"] = self.config[\"loss_function\"]\n",
    "        params[\"size\"] = self.config[\"size\"]\n",
    "        params[\"resample\"] = self.config[\"resample\"]\n",
    "        params[\"reorder\"] = self.config[\"reorder\"]\n",
    "        \n",
    "        params[\"transformations\"] = json.dumps(\n",
    "            self.config[\"transform_details\"], \n",
    "            indent=2, \n",
    "            separators=(',', ': ')\n",
    "        )\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _extract_epoch_metrics(self) -> dict[str, float]:\n",
    "        \"\"\"Extract metrics from the current epoch.\"\"\"\n",
    "        recorder = self.learn.recorder\n",
    "        \n",
    "        # Get custom metric names and values (skip 'epoch' and 'time')\n",
    "        metric_names = recorder.metric_names[2:]\n",
    "        raw_metric_values = recorder.log[2:]\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Process each metric, handling both scalars and tensors\n",
    "        for name, val in zip(metric_names, raw_metric_values):\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                if val.numel() == 1:\n",
    "                    # Single value tensor (like binary dice score)\n",
    "                    metrics[name] = float(val)\n",
    "                else:\n",
    "                    # Multi-element tensor (like multiclass dice scores)\n",
    "                    val_list = val.tolist() if hasattr(val, 'tolist') else list(val)\n",
    "                    # Log individual class scores\n",
    "                    for i, class_score in enumerate(val_list):\n",
    "                        metrics[f\"{name}_class_{i+1}\"] = float(class_score)\n",
    "                    # Log mean across classes\n",
    "                    metrics[f\"{name}_mean\"] = float(torch.mean(val))\n",
    "            else:\n",
    "                metrics[name] = float(val)\n",
    "        \n",
    "        # Handle loss values\n",
    "        if len(recorder.log) >= 2:\n",
    "            metrics['train_loss'] = float(recorder.log[1])\n",
    "            if len(recorder.log) >= 3:\n",
    "                metrics['valid_loss'] = float(recorder.log[2])\n",
    "                \n",
    "        return metrics\n",
    "    \n",
    "    def _save_model_artifacts(self, temp_dir: Path) -> None:\n",
    "        \"\"\"Save model weights, learner, and configuration as artifacts.\"\"\"\n",
    "        weights_path = temp_dir / \"weights\"\n",
    "        self.learn.save(str(weights_path))\n",
    "        \n",
    "        weights_file = f\"{weights_path}.pth\"\n",
    "        if os.path.exists(weights_file):\n",
    "            mlflow.log_artifact(weights_file, \"model\")\n",
    "        \n",
    "        learner_path = temp_dir / \"learner.pkl\"\n",
    "        self.learn.export(str(learner_path))\n",
    "        mlflow.log_artifact(str(learner_path), \"model\")\n",
    "        \n",
    "        config_path = temp_dir / \"inference_settings.pkl\"\n",
    "        store_variables(config_path, self.size, self.reorder, self.resample)\n",
    "        mlflow.log_artifact(str(config_path), \"config\")\n",
    "    \n",
    "    def _register_pytorch_model(self) -> None:\n",
    "        \"\"\"Register the PyTorch model with MLflow.\"\"\"\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=self.learn.model,\n",
    "            registered_model_name=self.model_name\n",
    "        )\n",
    "    \n",
    "    def before_fit(self) -> None:\n",
    "        \"\"\"Log hyperparameters before training starts.\"\"\"\n",
    "        params = self._extract_training_params()\n",
    "        mlflow.log_params(params)\n",
    "    \n",
    "    def after_epoch(self) -> None:\n",
    "        \"\"\"Log metrics after each epoch.\"\"\"\n",
    "        metrics = self._extract_epoch_metrics()\n",
    "        if metrics:\n",
    "            mlflow.log_metrics(metrics, step=self.learn.epoch)\n",
    "    \n",
    "    def after_fit(self) -> None:\n",
    "        \"\"\"Log model artifacts after training completion.\"\"\"\n",
    "        print(\"\\nTraining finished. Logging model artifacts to MLflow...\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            \n",
    "            self._save_model_artifacts(temp_path)\n",
    "            \n",
    "            self._register_pytorch_model()\n",
    "            \n",
    "        print(f\"MLflow run completed. Run ID: {mlflow.active_run().info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastmonai",
   "language": "python",
   "name": "fastmonai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
